#!/usr/bin/env python3
# encoding: utf-8
import os
import sys
import stat
import binascii
import fnmatch
import hashlib
import io
import math
from argparse import ArgumentParser
from collections import defaultdict
from nullroute.core import *

try:
    from wcwidth import wcswidth
except ImportError:
    wcswidth = lambda s: len(s)

# this doesn't need to be declared here
# I'm just doing so as a reminder that `opts` is global
opts = None
_isatty = None
_ttywidth = None

# header and hash caches, to avoid reading
# or hashing the same file twice
header_size = 512
file_sizes = {}     # path → size
file_headers = {}   # path → header
file_hashes = {}    # path → hash
total_files = 0
total_wasted = 0
files_removed = 0
size_removed = 0
dirs_skipped = 0

def isatty():
    global _isatty
    if _isatty is None:
        _isatty = sys.stderr.isatty()
    return _isatty

def ttywidth():
    global _ttywidth
    if _ttywidth is None:
        with os.popen("stty size", "r") as fh:
            line = fh.read().strip()
        rows, cols = line.split()
        _ttywidth = int(cols)
    return _ttywidth

_last_status = 0

def status(*args):
    if isatty() and not opts.verbose:
        msg = " ".join(args)
        msg = msg.replace("\n", " ")
        out = ""
        out += "\033[1G" # cursor to column 1
        out += "\033[0J" # erase below
        out += "\033[33m" + msg + "\033[m"
        lines = math.ceil(wcswidth(msg) / ttywidth())
        if lines > 1:
            out += "\033[%dA" % (lines-1) # cursor up 1
        sys.stderr.write(out)

        import time
        global _last_status
        now = time.time()
        if (not args) or (now - _last_status > 1):
            sys.stderr.flush()
            _last_status = now

def path_is_ignored(path):
    for filter in opts.ignore:
        if fnmatch.fnmatch(path, filter):
            return True
    return False

def path_is_removable(path):
    for filter in opts.keep:
        if fnmatch.fnmatch(path, filter):
            return False
    for filter in opts.remove:
        if fnmatch.fnmatch(path, filter):
            return True
    return False

def path_is_perishable(path):
    for filter in opts.only:
        if fnmatch.fnmatch(path, filter):
            return True
    return False

def enum_files(root_dir):
    global dirs_skipped
    ignores = {".git", ".hg", ".sync"}
    Core.debug("enumerating %r" % root_dir)
    if os.path.isdir(root_dir):
        for subdir, dirs, files in os.walk(root_dir):
            if ".nodupes" in files:
                Core.debug("skipping %r (found .nodupes marker)", subdir)
                dirs.clear()
                dirs_skipped += 1
                continue
            for item in dirs[:]:
                if item in ignores:
                    Core.debug("skipping %r/%r (ignored directory)", subdir, item)
                    dirs.remove(item)
            for name in files:
                path = os.path.join(subdir, name)
                if path_is_ignored(path):
                    continue
                yield path
    else:
        if not path_is_ignored(root_dir):
            yield root_dir
        else:
            Core.debug("skipping %r (root directory matches ignore)")

def get_header(path):
    if path not in file_headers:
        if opts.verbose:
            print("reading", path, file=sys.stderr)
        with open(path, "rb") as fh:
            file_headers[path] = hashlib.sha1(fh.read(header_size)).digest()
    return file_headers[path]

def hash_file(path, status_func=None):
    if path not in file_hashes:
        if opts.verbose:
            print("hashing", path, file=sys.stderr)
        h = hashlib.sha1()
        block = 4 * 1024 * 1024
        n_bytes = os.stat(path).st_size
        n_hashed = 0
        with open(path, "rb") as fh:
            buf = True
            while buf:
                buf = fh.read(block)
                h.update(buf)
                if status_func:
                    n_bytes += block
                    status_func(n_hashed / n_bytes * 100)
        file_hashes[path] = h.digest()
    return file_hashes[path]

def fmt_hash(hash):
    return binascii.b2a_hex(hash).decode("utf-8")

def fmt_size(nbytes, si=False):
    if nbytes == 0:
        return "0 bytes"
    prefixes = ".kMGTPEZYH"
    div = 1000 if si else 1024
    exp = int(math.log(nbytes, div))
    if exp == 0:
        return "%.1f bytes" % nbytes
    elif exp < len(prefixes):
        quot = nbytes / div**exp
        return "%.1f %sB" % (quot, prefixes[exp])
    else:
        exp = len(prefixes) - 1
        quot = nbytes / div**exp
        return "%f %sB" % (quot, prefixes[exp])
    return str(nbytes)

def unfmt_size(sz, si=False):
    if not sz:
        return -1
    prefixes = "kMGTPE"
    mult = 1000 if si else 1024
    pos = prefixes.index(sz[-1])
    val = float(sz[:-1])
    exp = pos + 1
    return val * (mult ** exp)

def find_duplicates(root_dirs):
    # dicts keeping duplicate items
    known_sizes = defaultdict(list)     # size → path[]
    known_headers = defaultdict(list)   # (size, header) → path[]
    known_hashes = defaultdict(list)    # (size, hash) → path[]

    n_size = 0
    n_head = 0
    n_hash = 0

    # find files identical in size
    for root_dir in root_dirs:
        for path in enum_files(root_dir):
            n_size += 1
            status("stat (%d)" % n_size, path)
            st = os.lstat(path)
            if not stat.S_ISREG(st.st_mode):
                continue
            #if st.st_size < opts.larger_than:
            #    continue
            file_sizes[path] = st.st_size
            known_sizes[st.st_size].append(path)

    status("stat/head")

    # find files identical in size and first `header_size` bytes
    head_todo = []

    for size, paths in known_sizes.items():
        if len(paths) > 1:
            head_todo += paths

    for path in sorted(head_todo):
        n_head += 1
        status("head (%d/%d)" % (n_head, len(head_todo)), path)
        header = get_header(path)
        known_headers[size, header].append(path)

    status("head/hash")

    # find files identical in size and hash
    for (size, header), paths in known_headers.items():
        if len(paths) < 2:
            n_hash += 1
            continue

        if size <= header_size:
            # optimization: don't compare by hash if
            # the entire contents are already known
            n_hash += len(paths)
            status()
            yield paths
            continue

        for path in paths:
            n_hash += 1
            status("hash (%d/%d)" % (n_hash, n_head), path)
            filehash = hash_file(path,
                lambda p: status("hash (%d/%d, %d%%)" % (n_hash, n_head, p), path))
            known_hashes[size, filehash].append(path)

    status()

    res = []
    for (size, filehash), paths in known_hashes.items():
        if len(paths) < 2:
            continue
        res.append(paths)
    res.sort()
    yield from res

cli_usage = "%(prog)s [options] <path>..."

cli_desc = """\
Finds files with duplicate data.
"""

cli_epilog = """\
This program ignores symlinks, special files, and the like. It also does not know about hardlinks; this might be added as an optimization later.
"""

if __name__ == "__main__":
    sys.stdout = io.TextIOWrapper(sys.stdout.detach(),
                                  encoding="utf-8",
                                  errors="surrogateescape")

    ap = ArgumentParser(usage=cli_usage, description=cli_desc, epilog=cli_epilog)
    ap.add_argument("-v", "--verbose",
                    dest="verbose", action="store_true", default=False,
                    help="show files as they are processed")
    ap.add_argument("-l", "--list",
                    dest="list", action="store_true", default=False,
                    help="output files as a sortable list")
    ap.add_argument("--ignore",
                    dest="ignore", action="append", default=[],
                    help="ignore matching paths entirely")
    ap.add_argument("--keep",
                    dest="keep", action="append", default=[],
                    help="always keep matching paths")
    ap.add_argument("--remove",
                    dest="remove", action="append", default=[],
                    help="automatically remove matching paths")
    ap.add_argument("--only",
                    dest="only", action="append", default=[],
                    help="only list files with duplicates matching given paths")
    ap.add_argument("--larger-than", dest="larger_than")
    ap.add_argument("path", nargs="*")

    opts = ap.parse_args()

    opts.larger_than = unfmt_size(opts.larger_than)

    root_dir = opts.path[:] or ["."]

    try:
        for paths in find_duplicates(root_dir):
            if opts.only and not any(path_is_perishable(p) for p in paths):
                continue
            paths.sort()
            size = file_sizes[paths[0]]
            hash = hash_file(paths[0])
            num = len(paths)
            wasted = size * (num - 1)
            if opts.list and opts.remove:
                for path in paths:
                    if num > 1 and path_is_removable(path):
                        print("rm -vf '%s'" % path)
                        num -= 1
            elif opts.list:
                for path in paths:
                    print(wasted, fmt_hash(hash), path)
            else:
                print("\033[38;5;11mDuplicates (%s wasted):\033[m" % fmt_size(wasted))
                for path in paths:
                    if num > 1 and path_is_removable(path):
                        print("   \033[1m\033[38;5;9m×\033[m", path)
                        os.unlink(path)
                        files_removed += 1
                        size_removed += size
                        num -= 1
                        wasted -= size
                        continue
                    print("    ", path)
            total_files += (num - 1)
            total_wasted += wasted
    except KeyboardInterrupt:
        status()
        Core.notice("scan interrupted")

    sys.stdout.flush()

    if opts.list and opts.remove:
        pass
    elif opts.verbose or opts.list:
        print("; %d files compared by header" % len(file_headers))
        print("; %d files compared by hash" % len(file_hashes))
        print("; %s wasted by %d duplicates" % (fmt_size(total_wasted), total_files))
        if dirs_skipped:
            print("; %d directories skipped" % dirs_skipped)
    else:
        if dirs_skipped:
            Core.notice("%d directories skipped" % dirs_skipped)
        if files_removed:
            Core.info("%s files removed, %s saved" % (files_removed, fmt_size(size_removed)))
        if total_files:
            Core.info("%s files remain, %s wasted" % (total_files, fmt_size(total_wasted)))
